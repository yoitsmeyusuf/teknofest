{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b8404f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Address normalization and matching pipeline with evaluation and CV\n",
    "import re, unicodedata\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Dependencies\n",
    "try:\n",
    "    from rapidfuzz import fuzz, process\n",
    "except ImportError:\n",
    "    import sys\n",
    "    !{sys.executable} -m pip -q install rapidfuzz\n",
    "    from rapidfuzz import fuzz, process\n",
    "\n",
    "try:\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import (\n",
    "        precision_score,\n",
    "        recall_score,\n",
    "        f1_score,\n",
    "        roc_auc_score,\n",
    "        accuracy_score,\n",
    "        classification_report,\n",
    "    )\n",
    "    from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "except ImportError:\n",
    "    import sys\n",
    "    !{sys.executable} -m pip -q install scikit-learn\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import (\n",
    "        precision_score,\n",
    "        recall_score,\n",
    "        f1_score,\n",
    "        roc_auc_score,\n",
    "        accuracy_score,\n",
    "        classification_report,\n",
    "    )\n",
    "    from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "\n",
    "# 1) Normalization\n",
    "ABBREV = [\n",
    "    (r\"\\bmah\\.?\\b\", \"mahalle\"),\n",
    "    (r\"\\bmh\\b\", \"mahalle\"),\n",
    "    (r\"\\bcad(de|d(e|e)?si)?\\b\", \"cadde\"),\n",
    "    (r\"\\bcad\\.?\\b\", \"cadde\"),\n",
    "    (r\"\\bcd\\.?\\b\", \"cadde\"),\n",
    "    (r\"\\bsok(ak)?\\b\", \"sokak\"),\n",
    "    (r\"\\bsk\\.?\\b\", \"sokak\"),\n",
    "    (r\"\\bapt\\.?\\b\", \"apartman\"),\n",
    "    (r\"\\bap\\.?\\b\", \"apartman\"),\n",
    "]\n",
    "TR_MAP = str.maketrans({\"ç\":\"c\",\"ğ\":\"g\",\"ı\":\"i\",\"ö\":\"o\",\"ş\":\"s\",\"ü\":\"u\",\"â\":\"a\",\"î\":\"i\",\"û\":\"u\"})\n",
    "PUNCT = re.compile(r\"[^a-z0-9\\s]\")\n",
    "MULTISPACE = re.compile(r\"\\s+\")\n",
    "\n",
    "def normalize_address(s: str) -> str:\n",
    "    s = s or \"\"\n",
    "    s = unicodedata.normalize(\"NFKD\", s.lower())\n",
    "    s = \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "    s = s.translate(TR_MAP)\n",
    "    s = s.replace(\"/\", \" \").replace(\"\\\\\", \" \").replace(\"-\", \" \").replace(\"_\", \" \")\n",
    "    for pat, repl in ABBREV:\n",
    "        s = re.sub(pat, repl, s)\n",
    "    s = re.sub(r\"\\b(no|kat|daire)\\s*[:#=.-]?\\s*\", r\" \\1 \", s)\n",
    "    s = PUNCT.sub(\" \", s)\n",
    "    s = MULTISPACE.sub(\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# 2) Gazetteer + fuzzy matching\n",
    "@dataclass\n",
    "class Gazetteer:\n",
    "    iller: List[str]\n",
    "    ilceler: List[str]\n",
    "    mahalleler: List[str]\n",
    "    sokaklar: List[str]\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # store normalized versions for matching (works with large lists)\n",
    "        self.iller_norm = [normalize_address(x) for x in self.iller]\n",
    "        self.ilceler_norm = [normalize_address(x) for x in self.ilceler]\n",
    "        self.mahalleler_norm = [normalize_address(x) for x in self.mahalleler]\n",
    "        self.sokaklar_norm = [normalize_address(x) for x in self.sokaklar]\n",
    "        # inverted token index for fast candidate pruning\n",
    "        from collections import defaultdict\n",
    "        self.idx_mahalle = defaultdict(set)\n",
    "        self.idx_sokak = defaultdict(set)\n",
    "        for i, s in enumerate(self.mahalleler_norm):\n",
    "            for t in s.split():\n",
    "                if len(t) >= 3:\n",
    "                    self.idx_mahalle[t].add(i)\n",
    "        for i, s in enumerate(self.sokaklar_norm):\n",
    "            for t in s.split():\n",
    "                if len(t) >= 3:\n",
    "                    self.idx_sokak[t].add(i)\n",
    "\n",
    "\n",
    "def fuzzy_best(query: str, choices: List[str], score_cutoff: int = 80) -> Tuple[Optional[str], int]:\n",
    "    if not query or not choices:\n",
    "        return None, 0\n",
    "    res = process.extractOne(query, choices, scorer=fuzz.WRatio, score_cutoff=score_cutoff)\n",
    "    if res is None:\n",
    "        return None, 0\n",
    "    match, score, _idx = res\n",
    "    return match, int(score)\n",
    "\n",
    "# 3) Phonetic (very light TR-compatible soundex)\n",
    "VOWELS = set(\"aeiou\")\n",
    "\n",
    "def turkish_soundex(word: str) -> str:\n",
    "    w = normalize_address(word)\n",
    "    if not w:\n",
    "        return \"\"\n",
    "    # keep first letter\n",
    "    first = w[0]\n",
    "    # simple mapping similar to Soundex groups\n",
    "    mapping = {\n",
    "        **{c:\"1\" for c in \"bfpv\"},\n",
    "        **{c:\"2\" for c in \"cgjkqsxz\"},\n",
    "        **{c:\"3\" for c in \"dt\"},\n",
    "        **{c:\"4\" for c in \"l\"},\n",
    "        **{c:\"5\" for c in \"mn\"},\n",
    "        **{c:\"6\" for c in \"r\"},\n",
    "    }\n",
    "    # drop vowels and h,w,y\n",
    "    code = []\n",
    "    prev = \"\"\n",
    "    for ch in w[1:]:\n",
    "        if ch in VOWELS or ch in \"hwy \":\n",
    "            digit = \"\"\n",
    "        else:\n",
    "            digit = mapping.get(ch, \"\")\n",
    "        if digit and digit != prev:\n",
    "            code.append(digit)\n",
    "            prev = digit\n",
    "    return (first + \"\".join(code) + \"0000\")[:4]\n",
    "\n",
    "def phonetic_similarity(a: str, b: str) -> float:\n",
    "    \"\"\"Return 1.0 if soundex codes match and strings are non-empty, else 0.0.\"\"\"\n",
    "    ca, cb = turkish_soundex(a), turkish_soundex(b)\n",
    "    if not ca or not cb:\n",
    "        return 0.0\n",
    "    return 1.0 if ca == cb else 0.0\n",
    "\n",
    "# 4) Extract parts via gazetteer with caching\n",
    "@dataclass\n",
    "class MatchResult:\n",
    "    il: Optional[str]\n",
    "    ilce: Optional[str]\n",
    "    mahalle: Optional[str]\n",
    "    sokak: Optional[str]\n",
    "    mahalle_score: int\n",
    "    sokak_score: int\n",
    "\n",
    "# simple cache to speed up repeated matches on same address\n",
    "GAZ_CACHE: Dict[Tuple[int, str], MatchResult] = {}\n",
    "\n",
    "MAX_CHOICES = 400\n",
    "\n",
    "def _candidate_list(query_tokens: List[str], norm_list: List[str], idx_map: Dict[str, set]) -> List[str]:\n",
    "    cand_idx = set()\n",
    "    for t in query_tokens:\n",
    "        cand_idx.update(idx_map.get(t, ()))\n",
    "        if len(cand_idx) >= MAX_CHOICES:\n",
    "            break\n",
    "    if not cand_idx:\n",
    "        # fallback to first MAX_CHOICES by frequency order\n",
    "        return norm_list[:MAX_CHOICES]\n",
    "    # materialize\n",
    "    out = [norm_list[i] for i in cand_idx]\n",
    "    # cap length deterministically\n",
    "    return out[:MAX_CHOICES]\n",
    "\n",
    "def gazetteer_match(addr_norm: str, gaz: Gazetteer, cutoff: int = 80) -> MatchResult:\n",
    "    key = (id(gaz), addr_norm)\n",
    "    if key in GAZ_CACHE:\n",
    "        return GAZ_CACHE[key]\n",
    "    q = addr_norm\n",
    "    q_tokens = [t for t in q.split() if len(t) >= 3]\n",
    "    il, _ = fuzzy_best(q, gaz.iller_norm, score_cutoff=cutoff)\n",
    "    ilce, _ = fuzzy_best(q, gaz.ilceler_norm, score_cutoff=cutoff)\n",
    "    # prune candidates for mahalle & sokak via inverted index\n",
    "    mah_choices = _candidate_list(q_tokens, gaz.mahalleler_norm, gaz.idx_mahalle)\n",
    "    sok_choices = _candidate_list(q_tokens, gaz.sokaklar_norm, gaz.idx_sokak)\n",
    "    mahalle, mahalle_score = fuzzy_best(q, mah_choices, score_cutoff=cutoff)\n",
    "    sokak, sokak_score = fuzzy_best(q, sok_choices, score_cutoff=cutoff)\n",
    "\n",
    "    def denorm(choice: Optional[str], raw: List[str], norm: List[str]) -> Optional[str]:\n",
    "        if choice is None:\n",
    "            return None\n",
    "        try:\n",
    "            idx = norm.index(choice)\n",
    "            return raw[idx]\n",
    "        except ValueError:\n",
    "            return choice\n",
    "\n",
    "    res = MatchResult(\n",
    "        il=denorm(il, gaz.iller, gaz.iller_norm),\n",
    "        ilce=denorm(ilce, gaz.ilceler, gaz.ilceler_norm),\n",
    "        mahalle=denorm(mahalle, gaz.mahalleler, gaz.mahalleler_norm),\n",
    "        sokak=denorm(sokak, gaz.sokaklar, gaz.sokaklar_norm),\n",
    "        mahalle_score=mahalle_score,\n",
    "        sokak_score=sokak_score,\n",
    "    )\n",
    "    GAZ_CACHE[key] = res\n",
    "    return res\n",
    "\n",
    "# Utilities\n",
    "# improved house number extraction: supports 12/3, 12A/3\n",
    "ADDR_NO_RE = re.compile(r\"\\bno\\s*(\\d+[a-zA-Z]?)(?:/(\\d+[a-zA-Z]?))?\\b\")\n",
    "\n",
    "def extract_house_no(s: str) -> Optional[str]:\n",
    "    m = ADDR_NO_RE.search(s)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def token_jaccard(a: str, b: str) -> float:\n",
    "    ta = set(a.split())\n",
    "    tb = set(b.split())\n",
    "    if not ta and not tb:\n",
    "        return 0.0\n",
    "    inter = len(ta & tb)\n",
    "    union = len(ta | tb)\n",
    "    return float(inter) / float(union) if union else 0.0\n",
    "\n",
    "# Feature engineering\n",
    "FEATURE_NAMES = [\n",
    "    \"il_eq\", \"ilce_eq\", \"mahalle_fuzzy\", \"sokak_fuzzy\", \"no_eq\",\n",
    "    \"ph_mahalle\", \"ph_sokak\", \"tok_jaccard\",\n",
    "]\n",
    "\n",
    "def pair_features(a: str, b: str, gaz: Gazetteer) -> np.ndarray:\n",
    "    a_norm, b_norm = normalize_address(a), normalize_address(b)\n",
    "    ma = gazetteer_match(a_norm, gaz)\n",
    "    mb = gazetteer_match(b_norm, gaz)\n",
    "    il_eq = int((ma.il or \"\") == (mb.il or \"\"))\n",
    "    ilce_eq = int((ma.ilce or \"\") == (mb.ilce or \"\"))\n",
    "    mahalle_sim = fuzz.WRatio(normalize_address(ma.mahalle or \"\"), normalize_address(mb.mahalle or \"\")) if (ma.mahalle and mb.mahalle) else 0\n",
    "    sokak_sim = fuzz.WRatio(normalize_address(ma.sokak or \"\"), normalize_address(mb.sokak or \"\")) if (ma.sokak and mb.sokak) else 0\n",
    "    no_a = extract_house_no(a_norm)\n",
    "    no_b = extract_house_no(b_norm)\n",
    "    no_eq = int((no_a or \"\") == (no_b or \"\"))\n",
    "    ph_mahalle = phonetic_similarity(ma.mahalle or \"\", mb.mahalle or \"\")\n",
    "    ph_sokak = phonetic_similarity(ma.sokak or \"\", mb.sokak or \"\")\n",
    "    tok_jac = token_jaccard(a_norm, b_norm)\n",
    "    return np.array([il_eq, ilce_eq, mahalle_sim, sokak_sim, no_eq, ph_mahalle, ph_sokak, tok_jac], dtype=np.float32)\n",
    "\n",
    "# Dataset -> features\n",
    "def build_feature_matrix(df_pairs: pd.DataFrame, gaz: Gazetteer) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    X = np.vstack([pair_features(a, b, gaz) for a, b in zip(df_pairs['a'], df_pairs['b'])])\n",
    "    y = df_pairs['label'].astype(int).values\n",
    "    return X, y\n",
    "\n",
    "# 6) Model with split, evaluation, and CV\n",
    "class AddressMatcher:\n",
    "    def __init__(self, n_estimators: int = 300, max_depth: int = 18, random_state: int = 42, n_jobs: int = -1):\n",
    "        self.clf = RandomForestClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            random_state=random_state,\n",
    "            n_jobs=n_jobs,\n",
    "            class_weight='balanced_subsample',\n",
    "        )\n",
    "        self.gaz: Optional[Gazetteer] = None\n",
    "        self.metrics_: Dict[str, float] = {}\n",
    "\n",
    "    def fit(self, df_pairs: pd.DataFrame, gaz: Gazetteer, test_size: float = 0.2, stratify: bool = True):\n",
    "        self.gaz = gaz\n",
    "        X, y = build_feature_matrix(df_pairs, gaz)\n",
    "        strat = y if stratify else None\n",
    "        X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=test_size, random_state=42, stratify=strat)\n",
    "        self.clf.fit(X_tr, y_tr)\n",
    "        self.metrics_ = self._evaluate(X_te, y_te)\n",
    "        return self\n",
    "\n",
    "    def _evaluate(self, X: np.ndarray, y: np.ndarray) -> Dict[str, float]:\n",
    "        proba = self.clf.predict_proba(X)[:, 1]\n",
    "        pred = (proba >= 0.5).astype(int)\n",
    "        out = {\n",
    "            'accuracy': accuracy_score(y, pred),\n",
    "            'precision': precision_score(y, pred, zero_division=0),\n",
    "            'recall': recall_score(y, pred, zero_division=0),\n",
    "            'f1': f1_score(y, pred, zero_division=0),\n",
    "        }\n",
    "        try:\n",
    "            out['roc_auc'] = roc_auc_score(y, proba)\n",
    "        except Exception:\n",
    "            out['roc_auc'] = float('nan')\n",
    "        return out\n",
    "\n",
    "    def cross_validate(self, df_pairs: pd.DataFrame, gaz: Gazetteer, k: int = 5) -> Dict[str, float]:\n",
    "        X, y = build_feature_matrix(df_pairs, gaz)\n",
    "        cv = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "        scores = {\n",
    "            'f1': cross_val_score(self.clf, X, y, cv=cv, scoring='f1').mean(),\n",
    "            'precision': cross_val_score(self.clf, X, y, cv=cv, scoring='precision').mean(),\n",
    "            'recall': cross_val_score(self.clf, X, y, cv=cv, scoring='recall').mean(),\n",
    "            'roc_auc': cross_val_score(self.clf, X, y, cv=cv, scoring='roc_auc').mean(),\n",
    "            'accuracy': cross_val_score(self.clf, X, y, cv=cv, scoring='accuracy').mean(),\n",
    "        }\n",
    "        return scores\n",
    "\n",
    "    def predict_proba(self, a: str, b: str) -> float:\n",
    "        x = pair_features(a, b, self.gaz).reshape(1, -1)\n",
    "        return float(self.clf.predict_proba(x)[0, 1])\n",
    "\n",
    "    def predict(self, a: str, b: str, threshold: float = 0.5) -> int:\n",
    "        return int(self.predict_proba(a, b) >= threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445022c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (848237, 2) cols: ['address', 'label']\n",
      "test shape: (217241, 2) cols: ['id', 'address']\n",
      "Loaded external ilceler: 894\n",
      "Mined gazetteer sizes -> ilceler: 894 mahalle: 5000 sokak: 5000\n"
     ]
    }
   ],
   "source": [
    "# Load dataset from data/ and mine a gazetteer from train (use external ilceler list)\n",
    "import os\n",
    "from collections import Counter, defaultdict\n",
    "iller = [\"Adana\", \"Adıyaman\", \"Afyonkarahisar\", \"Ağrı\", \"Aksaray\", \"Amasya\", \"Ankara\", \"Antalya\", \"Ardahan\", \"Artvin\", \"Aydın\", \"Balıkesir\", \"Bartın\", \"Batman\", \"Bayburt\", \"Bilecik\", \"Bingöl\", \"Bitlis\", \"Bolu\", \"Burdur\", \"Bursa\", \"Çanakkale\", \"Çankırı\", \"Çorum\", \"Denizli\", \"Diyarbakır\", \"Düzce\", \"Edirne\", \"Elazığ\", \"Erzincan\", \"Erzurum\", \"Eskişehir\", \"Gaziantep\", \"Giresun\", \"Gümüşhane\", \"Hakkâri\", \"Hatay\", \"Iğdır\", \"Isparta\", \"İstanbul\", \"İzmir\", \"Kahramanmaraş\", \"Karabük\", \"Karaman\", \"Kars\", \"Kastamonu\", \"Kayseri\", \"Kilis\", \"Kırıkkale\", \"Kırklareli\", \"Kırşehir\", \"Kocaeli\", \"Konya\", \"Kütahya\", \"Malatya\", \"Manisa\", \"Mardin\", \"Mersin\", \"Muğla\", \"Muş\", \"Nevşehir\", \"Niğde\", \"Ordu\", \"Osmaniye\", \"Rize\", \"Sakarya\", \"Samsun\", \"Şanlıurfa\", \"Siirt\", \"Sinop\", \"Sivas\", \"Şırnak\", \"Tekirdağ\", \"Tokat\", \"Trabzon\", \"Tunceli\", \"Uşak\", \"Van\", \"Yalova\", \"Yozgat\", \"Zonguldak\"]\n",
    "DATA_DIR = '/home/yusuf/teknofest/data'\n",
    "ILCELER_TXT = '/home/yusuf/teknofest/ilceler_unique.txt'\n",
    "\n",
    "# helpers\n",
    "def read_csv_safe(path: str) -> pd.DataFrame:\n",
    "    return pd.read_csv(path, dtype=str, keep_default_na=False, encoding='utf-8', engine='python')\n",
    "\n",
    "def read_list_from_txt(path: str) -> List[str]:\n",
    "    if not os.path.exists(path):\n",
    "        return []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        lines = [ln.strip() for ln in f.readlines()]\n",
    "    return [ln for ln in lines if ln]\n",
    "\n",
    "# Locate files\n",
    "train_path = os.path.join(DATA_DIR, 'train.csv')\n",
    "test_path = os.path.join(DATA_DIR, 'test.csv')\n",
    "assert os.path.exists(train_path) and os.path.exists(test_path), 'train.csv/test.csv not found under data/'\n",
    "\n",
    "train_df = read_csv_safe(train_path)\n",
    "test_df = read_csv_safe(test_path)\n",
    "print('train shape:', train_df.shape, 'cols:', list(train_df.columns))\n",
    "print('test shape:', test_df.shape, 'cols:', list(test_df.columns))\n",
    "\n",
    "# Expected columns: train: address, label; test: id, address\n",
    "addr_col = 'address'\n",
    "label_col = 'label'\n",
    "assert addr_col in train_df.columns and label_col in train_df.columns\n",
    "assert addr_col in test_df.columns\n",
    "\n",
    "# Normalize once\n",
    "train_df['address_norm'] = train_df[addr_col].apply(normalize_address)\n",
    "test_df['address_norm'] = test_df[addr_col].apply(normalize_address)\n",
    "\n",
    "# External ilceler list\n",
    "ilceler_ext = read_list_from_txt(ILCELER_TXT)\n",
    "print('Loaded external ilceler:', len(ilceler_ext))\n",
    "\n",
    "# Mine gazetteer candidates from normalized text\n",
    "KEYS = {'mahalle', 'sokak', 'cadde', 'bulvar'}\n",
    "\n",
    "def mine_gazetteer(\n",
    "    df: pd.DataFrame,\n",
    "    external_ilceler: Optional[List[str]] = None,\n",
    "    max_items: int = 5000,\n",
    "    min_freq: int = 2,\n",
    "    use_bigrams: bool = True,\n",
    ") -> Gazetteer:\n",
    "    mah_cand = Counter()\n",
    "    sok_cand = Counter()\n",
    "    for s in df['address_norm'].values:\n",
    "        toks = s.split()\n",
    "        for i, t in enumerate(toks):\n",
    "            if t in KEYS and i > 0:\n",
    "                prev1 = toks[i-1]\n",
    "                if prev1:\n",
    "                    if t == 'mahalle':\n",
    "                        mah_cand[prev1] += 1\n",
    "                    elif t in {'sokak', 'cadde', 'bulvar'}:\n",
    "                        sok_cand[prev1] += 1\n",
    "                if use_bigrams and i > 1:\n",
    "                    prev2 = toks[i-2] + ' ' + toks[i-1]\n",
    "                    if t == 'mahalle':\n",
    "                        mah_cand[prev2] += 1\n",
    "                    elif t in {'sokak', 'cadde', 'bulvar'}:\n",
    "                        sok_cand[prev2] += 1\n",
    "    # apply min frequency and top-k\n",
    "    mahalle_items = [(w, c) for w, c in mah_cand.items() if c >= min_freq]\n",
    "    sokak_items = [(w, c) for w, c in sok_cand.items() if c >= min_freq]\n",
    "    mahalle_items.sort(key=lambda x: -x[1])\n",
    "    sokak_items.sort(key=lambda x: -x[1])\n",
    "    mahalleler = [w for w, _ in mahalle_items[:max_items]]\n",
    "    sokaklar = [w for w, _ in sokak_items[:max_items]]\n",
    "    # dedupe and drop empties\n",
    "    mahalleler = [m for i, m in enumerate(mahalleler) if m and m not in mahalleler[:i]]\n",
    "    sokaklar = [m for i, m in enumerate(sokaklar) if m and m not in sokaklar[:i]]\n",
    "    ilceler = external_ilceler or []\n",
    "    return Gazetteer(iller=iller, ilceler=ilceler, mahalleler=mahalleler, sokaklar=sokaklar)\n",
    "\n",
    "# build gazetteer\n",
    "GAZ_CACHE.clear()\n",
    "mined_gaz = mine_gazetteer(train_df, external_ilceler=ilceler_ext, max_items=3000, min_freq=3, use_bigrams=False)\n",
    "print('Mined gazetteer sizes -> ilceler:', len(mined_gaz.ilceler), 'mahalle:', len(mined_gaz.mahalleler), 'sokak:', len(mined_gaz.sokaklar))\n",
    "\n",
    "# Address-level split to avoid leakage\n",
    "addr_by_label = train_df.groupby(label_col)['address_norm'].apply(list)\n",
    "unique_pairs = set()  # for dedupe pairs later\n",
    "\n",
    "def make_pairs_from_groups(groups: Dict[str, List[str]], max_pos_per_label: int = 400) -> List[Tuple[str, str, int]]:\n",
    "    from itertools import combinations\n",
    "    pairs = []\n",
    "    for lab, addrs in groups.items():\n",
    "        subset = addrs[:max_pos_per_label]\n",
    "        for a_idx in range(len(subset)):\n",
    "            for b_idx in range(a_idx + 1, len(subset)):\n",
    "                a, b = subset[a_idx], subset[b_idx]\n",
    "                if a == b:\n",
    "                    continue\n",
    "                key = (min(a, b), max(a, b), 1)\n",
    "                if key in unique_pairs:\n",
    "                    continue\n",
    "                unique_pairs.add(key)\n",
    "                pairs.append((a, b, 1))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1a389d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pairs_df: (39243085, 3) pos: 39241085 neg: 2000\n"
     ]
    }
   ],
   "source": [
    "# Build labeled pairs with address-level split to avoid leakage; add balanced negatives\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "\n",
    "# Split labels first, then create pairs inside train split only\n",
    "labels = train_df[label_col].unique().tolist()\n",
    "labels.sort()\n",
    "\n",
    "# Stratified split at label level: 80/20\n",
    "from sklearn.model_selection import train_test_split as _tts\n",
    "label_tr, label_te = _tts(labels, test_size=0.2, random_state=42)\n",
    "\n",
    "by_label = train_df.groupby(label_col)['address_norm'].apply(list)\n",
    "\n",
    "# Positive pairs from TRAIN labels only\n",
    "pos_pairs = []\n",
    "for lab in label_tr:\n",
    "    addrs = by_label.get(lab, [])\n",
    "    if not addrs:\n",
    "        continue\n",
    "    max_per_label = 200\n",
    "    for a, b in combinations(addrs[:max_per_label], 2):\n",
    "        if a == b:\n",
    "            continue\n",
    "        pos_pairs.append((a, b, 1))\n",
    "\n",
    "# Balanced negative sampling: sample pairs across two different labels\n",
    "rng = np.random.default_rng(42)\n",
    "neg_pairs = []\n",
    "# create a map label->addresses subset\n",
    "addr_map = {lab: by_label.get(lab, [])[:200] for lab in label_tr}\n",
    "label_tr_list = [lab for lab in label_tr if len(addr_map[lab]) >= 1]\n",
    "num_negs_target = len(pos_pairs)\n",
    "while len(neg_pairs) < num_negs_target and len(label_tr_list) >= 2:\n",
    "    la, lb = rng.choice(label_tr_list, size=2, replace=False)\n",
    "    a_list, b_list = addr_map[la], addr_map[lb]\n",
    "    if not a_list or not b_list:\n",
    "        continue\n",
    "    a = a_list[rng.integers(0, len(a_list))]\n",
    "    b = b_list[rng.integers(0, len(b_list))]\n",
    "    if a == b:\n",
    "        continue\n",
    "    neg_pairs.append((a, b, 0))\n",
    "\n",
    "pairs_df = pd.DataFrame(pos_pairs + neg_pairs, columns=['a', 'b', 'label']).drop_duplicates()\n",
    "print('pairs_df:', pairs_df.shape, 'pos:', (pairs_df.label==1).sum(), 'neg:', (pairs_df.label==0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92956756",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Train/evaluate with mined gazetteer and constructed pairs\u001b[39;00m\n\u001b[32m      2\u001b[39m matcher2 = AddressMatcher()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mmatcher2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpairs_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlabel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlabel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmined_gaz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mHoldout metrics (mined gazetteer):\u001b[39m\u001b[33m'\u001b[39m, matcher2.metrics_)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mCV(3) metrics:\u001b[39m\u001b[33m'\u001b[39m, matcher2.cross_validate(pairs_df, mined_gaz, k=\u001b[32m3\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 222\u001b[39m, in \u001b[36mAddressMatcher.fit\u001b[39m\u001b[34m(self, df_pairs, gaz, test_size, stratify)\u001b[39m\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, df_pairs: pd.DataFrame, gaz: Gazetteer, test_size: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m0.2\u001b[39m, stratify: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    221\u001b[39m     \u001b[38;5;28mself\u001b[39m.gaz = gaz\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m     X, y = \u001b[43mbuild_feature_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_pairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgaz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    223\u001b[39m     strat = y \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    224\u001b[39m     X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=test_size, random_state=\u001b[32m42\u001b[39m, stratify=strat)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 209\u001b[39m, in \u001b[36mbuild_feature_matrix\u001b[39m\u001b[34m(df_pairs, gaz)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbuild_feature_matrix\u001b[39m(df_pairs: pd.DataFrame, gaz: Gazetteer) -> Tuple[np.ndarray, np.ndarray]:\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m     X = np.vstack([\u001b[43mpair_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgaz\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m a, b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(df_pairs[\u001b[33m'\u001b[39m\u001b[33ma\u001b[39m\u001b[33m'\u001b[39m], df_pairs[\u001b[33m'\u001b[39m\u001b[33mb\u001b[39m\u001b[33m'\u001b[39m])])\n\u001b[32m    210\u001b[39m     y = df_pairs[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m].astype(\u001b[38;5;28mint\u001b[39m).values\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 194\u001b[39m, in \u001b[36mpair_features\u001b[39m\u001b[34m(a, b, gaz)\u001b[39m\n\u001b[32m    192\u001b[39m a_norm, b_norm = normalize_address(a), normalize_address(b)\n\u001b[32m    193\u001b[39m ma = gazetteer_match(a_norm, gaz)\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m mb = \u001b[43mgazetteer_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgaz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m il_eq = \u001b[38;5;28mint\u001b[39m((ma.il \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m) == (mb.il \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m    196\u001b[39m ilce_eq = \u001b[38;5;28mint\u001b[39m((ma.ilce \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m) == (mb.ilce \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 148\u001b[39m, in \u001b[36mgazetteer_match\u001b[39m\u001b[34m(addr_norm, gaz, cutoff)\u001b[39m\n\u001b[32m    146\u001b[39m il, _ = fuzzy_best(q, gaz.iller_norm, score_cutoff=cutoff)\n\u001b[32m    147\u001b[39m ilce, _ = fuzzy_best(q, gaz.ilceler_norm, score_cutoff=cutoff)\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m mahalle, mahalle_score = \u001b[43mfuzzy_best\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgaz\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmahalleler_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore_cutoff\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcutoff\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m sokak, sokak_score = fuzzy_best(q, gaz.sokaklar_norm, score_cutoff=cutoff)\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdenorm\u001b[39m(choice: Optional[\u001b[38;5;28mstr\u001b[39m], raw: List[\u001b[38;5;28mstr\u001b[39m], norm: List[\u001b[38;5;28mstr\u001b[39m]) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 90\u001b[39m, in \u001b[36mfuzzy_best\u001b[39m\u001b[34m(query, choices, score_cutoff)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m query \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m choices:\n\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m res = \u001b[43mprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextractOne\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchoices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfuzz\u001b[49m\u001b[43m.\u001b[49m\u001b[43mWRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore_cutoff\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscore_cutoff\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/rapidfuzz/process_cpp_impl.pyx:848\u001b[39m, in \u001b[36mrapidfuzz.process_cpp_impl.extractOne\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/rapidfuzz/process_cpp_impl.pyx:718\u001b[39m, in \u001b[36mrapidfuzz.process_cpp_impl.extractOne_list\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/rapidfuzz/process_cpp_impl.pyx:535\u001b[39m, in \u001b[36mrapidfuzz.process_cpp_impl.extractOne_list_f64\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Train/evaluate with mined gazetteer and constructed pairs\n",
    "matcher2 = AddressMatcher()\n",
    "matcher2.fit(pairs_df.rename(columns={'label':'label'}), mined_gaz, test_size=0.2)\n",
    "print('Holdout metrics (mined gazetteer):', matcher2.metrics_)\n",
    "print('CV(3) metrics:', matcher2.cross_validate(pairs_df, mined_gaz, k=3))\n",
    "\n",
    "# Evaluate generalization on HELD-OUT labels (simple sanity):\n",
    "# Create some pairs from held-out labels and score using the already-fit model\n",
    "held_pos, held_neg = [], []\n",
    "for lab in label_te[:50]:  # limit for speed\n",
    "    addrs = by_label.get(lab, [])\n",
    "    if len(addrs) >= 2:\n",
    "        held_pos.append((addrs[0], addrs[1], 1))\n",
    "# negatives from two different held-out labels\n",
    "if len(label_te) >= 2:\n",
    "    labs = list(label_te)\n",
    "    for i in range(min(50, len(labs)-1)):\n",
    "        la, lb = labs[i], labs[(i+1) % len(labs)]\n",
    "        a_list = by_label.get(la, [])\n",
    "        b_list = by_label.get(lb, [])\n",
    "        if a_list and b_list:\n",
    "            held_neg.append((a_list[0], b_list[0], 0))\n",
    "\n",
    "if held_pos or held_neg:\n",
    "    held_df = pd.DataFrame(held_pos + held_neg, columns=['a','b','label'])\n",
    "    Xh, yh = build_feature_matrix(held_df, mined_gaz)\n",
    "    proba = matcher2.clf.predict_proba(Xh)[:,1]\n",
    "    pred = (proba >= 0.5).astype(int)\n",
    "    print('Held-out labels metrics:', {\n",
    "        'accuracy': accuracy_score(yh, pred),\n",
    "        'precision': precision_score(yh, pred, zero_division=0),\n",
    "        'recall': recall_score(yh, pred, zero_division=0),\n",
    "        'f1': f1_score(yh, pred, zero_division=0),\n",
    "    })\n",
    "\n",
    "# Example: score similarity between two arbitrary test rows\n",
    "if len(test_df) >= 2:\n",
    "    a = test_df.iloc[0]['address']\n",
    "    b = test_df.iloc[1]['address']\n",
    "    print('Example test pair prob:', matcher2.predict_proba(a, b))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
